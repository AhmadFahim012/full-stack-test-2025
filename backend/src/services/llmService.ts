import { v4 as uuidv4 } from 'uuid';

export interface LLMResponse {
  id: string;
  message: string;
  timestamp: string;
  model: string;
}

export interface LLMRequest {
  message: string;
  chatHistory?: Array<{
    role: 'user' | 'assistant';
    content: string;
  }>;
}

// Simulated LLM responses with variety
const SIMULATED_RESPONSES = [
  "I understand your question and I'd be happy to help you with that. Let me provide you with a comprehensive answer that covers the key aspects of what you're asking about. This is a simulated response from an AI assistant that would normally be generated by a large language model.",
  
  "That's an interesting topic you've brought up. Based on the context you've provided, I can offer several perspectives on this matter. Here's what I think you should consider: First, there are multiple factors at play here that need to be addressed. Second, the approach you're considering has both advantages and potential drawbacks. Finally, I'd recommend taking a systematic approach to evaluate all your options before making a decision.",
  
  "Thank you for sharing that with me. I can see why this would be important to you. Let me break down my thoughts on this topic: The situation you're describing is quite common, and there are several proven strategies that have worked well for others in similar circumstances. I'd suggest starting with a thorough analysis of your current situation, then identifying the key constraints and opportunities available to you.",
  
  "I appreciate you taking the time to explain this to me. This is definitely a complex topic that requires careful consideration. From what I can gather, you're dealing with multiple interconnected factors that all need to be balanced. Here's my assessment: The primary challenge seems to be finding the right balance between different competing priorities. I'd recommend creating a structured approach that allows you to evaluate each component systematically while keeping the overall objective in mind.",
  
  "That's a great question, and I can see why it's important to you. Let me provide you with a detailed response that should help clarify the situation. Based on my understanding of the context, there are several key points to consider: First, the current state of affairs presents both opportunities and challenges. Second, the timeline you're working with will significantly impact the available options. Third, the resources at your disposal will determine which approaches are most viable.",
  
  "I understand your concern, and I want to make sure I give you a thorough and helpful response. This is a nuanced topic that requires careful analysis. Here's what I think: The approach you're considering has merit, but there are some important factors to weigh. I'd suggest starting with a clear definition of your goals, then mapping out the various paths that could lead you there. Each option will have different trade-offs in terms of time, resources, and potential outcomes."
];

export class LLMService {
  private static instance: LLMService;

  private constructor() {}

  public static getInstance(): LLMService {
    if (!LLMService.instance) {
      LLMService.instance = new LLMService();
    }
    return LLMService.instance;
  }

  /**
   * Simulates an LLM API call with realistic delay and response
   * @param request - The user's message and optional chat history
   * @returns Promise<LLMResponse> - Simulated AI response
   */
  public async generateResponse(request: LLMRequest): Promise<LLMResponse> {
    // Simulate network delay and processing time (10-20 seconds)
    const delay = this.getRandomDelay(10000, 20000);
    
    console.log(`LLM Service.`);
    
    await this.sleep(delay);

    // Select a random response from our simulated responses
    const randomResponse = SIMULATED_RESPONSES[
      Math.floor(Math.random() * SIMULATED_RESPONSES.length)
    ];

    // Add some context awareness based on the user's message
    const contextualResponse = this.addContextToResponse(request.message, randomResponse);

    const response: LLMResponse = {
      id: uuidv4(),
      message: contextualResponse,
      timestamp: new Date().toISOString(),
      model: 'gpt-4-simulated'
    };

    console.log(`LLM Service completed`);
    return response;
  }

  private getRandomDelay(min: number, max: number): number {
    return Math.floor(Math.random() * (max - min + 1)) + min;
  }
  private sleep(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }

  /**
   * Adds some basic context awareness to the response
   */
  private addContextToResponse(userMessage: string, baseResponse: string): string {
    const lowerMessage = userMessage.toLowerCase();
    
    // Simple keyword-based context enhancement
    if (lowerMessage.includes('help') || lowerMessage.includes('how')) {
      return `I'd be happy to help you with that! ${baseResponse}`;
    }
    
    if (lowerMessage.includes('explain') || lowerMessage.includes('what')) {
      return `Let me explain that for you. ${baseResponse}`;
    }
    
    if (lowerMessage.includes('problem') || lowerMessage.includes('issue')) {
      return `I understand you're facing a challenge. ${baseResponse}`;
    }
    
    if (lowerMessage.includes('thank') || lowerMessage.includes('thanks')) {
      return `You're very welcome! ${baseResponse}`;
    }
    
    return baseResponse;
  }

  /**
   * Simulates a streaming response (for future enhancement)
   */
  public async *generateStreamingResponse(request: LLMRequest): AsyncGenerator<string, void, unknown> {
    const response = await this.generateResponse(request);
    const words = response.message.split(' ');
    
    for (let i = 0; i < words.length; i++) {
      yield words[i] + (i < words.length - 1 ? ' ' : '');
      // Small delay between words to simulate streaming
      await this.sleep(50);
    }
  }
}
